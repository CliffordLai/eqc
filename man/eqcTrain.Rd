% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/eqcTrain.R
\name{eqcTrain}
\alias{eqcTrain}
\title{Train Ensemble Quantile Classifier}
\usage{
eqcTrain(train, cl.train, thetaList, method = c("qc", "glmnet", "svm",
  "multiclass"), skew.correct = c("Galton", "Kelley", "skewness",
  "none"), alpha = 0, lambda = c(0.5, 0.1, 0.005, 0.001, 5e-04, 1e-04),
  kernel = c("linear", "polynomial", "radial", "sigmoid"),
  cost = c(0.5, 1), ..., tuneControl = list())
}
\arguments{
\item{train}{A \code{n*p} matrix containg \code{n} observations of \code{p} variables for training.}

\item{cl.train}{A vector of length \code{n} containing the class labels 1,2,....
When there are more than two classes, \code{method} = "multiclass" should be used.}

\item{thetaList}{A matrix of \code{p} columns containing candidate probabilities of quantiles as rows.}

\item{method}{Ensemble method of \code{p} quantile-based classifiers. See details.}

\item{skew.correct}{Skewness measures applied to correct the skewness direction of the variables.
The possibile choices are: Galton's skewness (default),
Kelley's skewness, the conventional skewness index based on the third standardized moment and
no correction.}

\item{alpha}{The elasticnet mixing parameter used in \code{\link[glmnet]{glmnet}}, with \eqn{0\le \alpha \le 1}.
\code{alpha=1} is the lasso penalty, and \code{alpha=0} is the ridge penalty.
The default is 0.}

\item{lambda}{A user supplied lambda decreasing sequence used in \code{\link[glmnet]{glmnet}}
or for \code{method} = "multiclass".
The default is \code{c(0.5,0.1,0.005,0.001,0.0005,0.0001)}.}

\item{kernel}{The kernel used in training and predicting a SVM. The default is "linear".}

\item{cost}{A set of positive ‘C’-constants of the regularization term used in \code{\link[e1071]{svm}}.
The default is \code{c(0.5,1)}.}

\item{...}{Further arguments to be passed to \code{\link[glmnet]{glmnet}} or \code{\link[e1071]{svm}}.}

\item{tuneControl}{A list of control parameters for tuning the model. See details.}
}
\value{
\code{eqcTrain} produces an object of class "eqcTrain" is a list containing the following components:

\item{fitted}{A (large) list where each component contains the return values of the ensemble \code{method}
applied on the quantile-transformed \code{X} with respect to each row of \code{thetaList},
or the ensemble quantile classifer tuned by the cross-validation if \code{tuneControl} is specified.}
\item{K}{The number of classes.}
\item{method}{The ensemble method.}
\item{alpha}{The elasticnet mixing parameter used in \code{\link[glmnet]{glmnet}}.}
\item{lambda}{A lambda decreasing sequence used in \code{\link[glmnet]{glmnet}}.}
\item{kernel}{The kernel used in training and predicting a SVM.}
\item{cost}{A set of positive ‘C’-constants of the regularization term used in \code{\link[e1071]{svm}}.}
\item{skew.correct}{The input type of skewness correction.}
\item{thetaList}{A matrix of p columns containing candidate probabilities of quantiles as rows.}
\item{qList}{A 3-D array where qList[k,,] is a matrix of p columns containing
sample quantiles of Class k at each candidate probabilities.}
\item{signSkew}{A vector of length \code{p} containing flip sign of each variable.}
\item{zeroVar}{A list of positions of constant variables for each quantile-trasformed \code{X}
with respect to each row of \code{thetaList}.}
\item{CVmeasure}{A matrix consists of cross-validated \code{type.measure} for each combinations of tuning parameters, where
rows specify \code{theta} and columns specify \code{lambda} or \code{cost}.}
\item{cvparameter}{A list contains \code{theta}, \code{lambda} and \code{cost} selected by the cross-validation.}
\item{cvfold}{A list contains \code{nfolds}, \code{fold.seed}, \code{foldid} and \code{type.measure}.}
}
\description{
\code{eqcTrain} trains and tunes ensemble quantile classifiers.
Currently there are three ensemble ways.
}
\details{
\code{eqcTrain} trains ensemble quantile classifiers
by applying selected linear classification method
on every quantile-based transformed data with respect to each row of \code{thetaList}.
An illurastration of how this work can be found in the example of \code{\link{quantileTransform}}.

Currently, there are three ensemble methods specified by the argument \code{method}:
\code{"qc"} fits linear ensemble quantile classifier with equal weights.
\code{"glmnet"} fits linear ensemble quantile classifier with coefficients fitted by
a penalzied logistic regression \code{\link[glmnet]{glmnet}}.
\code{"svm"} fits linear ensemble quantile classifier with coefficients fitted by a SVM \code{\link[e1071]{svm}}.
\code{"multiclass"} fits linear ensemble quantile classifier with L2 regularized softmax rule for multiclass.

The argument \code{tuneControl} specifies whether the model is tuned or not.
It is a list that can supply any of the following components:

\code{nfolds}

Number of folds - default is \code{NULL}, meaning no tuning.

\code{fold.seed}

An optional random seed for generating the folds.

\code{foldid}

An optional vector of values between 1 and \code{nfold} identifying what fold each observation is in.
If supplied, it overrides \code{nfold}.

\code{type.measure}

Loss used for cross-validation. The default is "me" for misclassification error,
or "auc" for the area under the ROC curve if \code{method} supports probablity outputs.

\code{ncpu}

number of compute nodes for doing the cross-validatoin with parallel. Default is 1.
}
\examples{
# Divide data into training set and test set randomly
data(wdbc)
set.seed(193)
trainIndex <- sample(c(rep(TRUE,2),rep(FALSE,1)),nrow(wdbc),replace=TRUE)
train <- as.matrix(wdbc[trainIndex,-1])
cl.train <- wdbc[trainIndex,1]
test <- as.matrix(wdbc[!trainIndex,-1])
cl.test <- wdbc[!trainIndex,1]
p <- ncol(train)

# Tuning parameters
thetaList <- rbind(rep(0.3,p),rep(0.4,p),rep(0.5,p))

# Fit the tuned EQC (QC)
fit1 <- eqcTrain(train,cl.train,
                 thetaList=thetaList,
                 method = "qc",
                 tuneControl = list(nfolds=5,fold.seed=117))
fit1$cvparameter #Selected tuning parameters
acc1 <- mean(predict(fit1,newdata = test,type = "class")[[1]]==cl.test)
acc1 #0.9261364

# Fit the tuned EQC with a ridge logistic regression
lambda <- c(3,1,0.1,0.01,0.001,0.0005,0.0001)
fit2 <- eqcTrain(train,cl.train,
                 thetaList=thetaList,
                 method = "glmnet",
                 alpha = 0,lambda = lambda,
                 tuneControl = list(nfolds=5,fold.seed=117),
                 lower.limits=0, upper.limits=Inf)
fit2$cvparameter #Selected tuning parameters
acc2 <- mean(predict(fit2,newdata = test,type = "class")[[1]]==cl.test)
acc2 #0.9772727

# Fit the tuned EQC with a linear SVM
cost <- 2^(0:3)
fit3 <- eqcTrain(train,cl.train,
                 thetaList=thetaList,
                 method = "svm",
                 kernel = "linear",cost = cost,
                 tuneControl = list(nfolds=5,fold.seed=117))
fit3$cvparameter #Selected tuning parameters
acc3 <- mean(predict(fit3,newdata = test,type = "class")[[1]]==cl.test)
acc3 #0.9772727

#-----------------------Multiclass EQC-----------------------#
data(iris)
X <- as.matrix(iris[,1:4])
cl <- as.numeric(iris$Species)
n <- nrow(X)
p <- ncol(X)

# Split train and test
set.seed(193)
trainIndex <- sample(c(TRUE,TRUE,FALSE),size = n,replace = TRUE)
train <- X[trainIndex,]
cl.train <- cl[trainIndex]
test <- X[!trainIndex,]
cl.test <- cl[!trainIndex]

# Tuning parameters
thetaList <- rbind(rep(0.3,p),rep(0.4,p),rep(0.5,p))
lambda <- c(0.1,0.01)

# Fit
fit4 <- eqcTrain(train,cl.train,
                 thetaList=thetaList,
                 method = "multiclass",
                 lambda = lambda,
                 tuneControl = list(nfolds=5,fold.seed=117))
fit4$cvparameter$thetaCV[1]
# Predict
pred4 <- predict(fit4,newdata = test,type = "class")[[1]]
acc4 <- mean(pred4==cl.test)
acc4 #0.9387755

# Compared with SVM without tuning
model <- e1071::svm(x=train,y=factor(cl.train))
mean(predict(model,test)==cl.test) #0.9183673

}
\references{
Lai
}
\seealso{
\code{\link{quantileTransform}},
\code{\link[glmnet]{glmnet}}, \code{\link[e1071]{svm}}.
}
\author{
Yuanhao Lai
}
